<!DOCTYPE html>
<html
  lang="zh-cn"
  itemscope
  itemtype="http://schema.org/WebPage"
>
  <head>
    <meta charset="utf-8" />
    <meta http-equiv="X-UA-Compatible" content="IE=edge" />
    <title>
          用FP8训练大模型有多香？微软：比BF16快64%，省42%内存 - 区块大全
        </title>
    

<meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=yes"/>

<meta name="MobileOptimized" content="width"/>
<meta name="HandheldFriendly" content="true"/>


<meta name="applicable-device" content="pc,mobile">

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">

<meta name="mobile-web-app-capable" content="yes">

<meta name="author" content="佚名" /><meta name="description" content="低精度训练是大模型训练中扩展模型大小，节约训练成本的最关键技术之一。" />

  <meta name="keywords" content="区块大全, 区块链, 区块链资讯, 区块链快讯, 区块链新闻, 比特币行情" />






<meta name="generator" content="Hugo 0.120.4" />


<link rel="canonical" href="/post/62383/" />





<link rel="icon" href="/favicon.ico" />











<link rel="stylesheet" href="/sass/jane.min.d8d87b982993a745e5e7b6a6cbf257be8c3e82aab5e485f0908ad7e6c3501ab2.css" integrity="sha256-2Nh7mCmTp0Xl57amy/JXvow&#43;gqq15IXwkIrX5sNQGrI=" media="screen" crossorigin="anonymous">







<meta property="og:title" content="用FP8训练大模型有多香？微软：比BF16快64%，省42%内存" />
<meta property="og:description" content="低精度训练是大模型训练中扩展模型大小，节约训练成本的最关键技术之一。" />
<meta property="og:type" content="article" />
<meta property="og:url" content="/post/62383/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2023-11-23T00:00:00+00:00" />
<meta property="article:modified_time" content="2023-11-23T00:00:00+00:00" />

<meta itemprop="name" content="用FP8训练大模型有多香？微软：比BF16快64%，省42%内存">
<meta itemprop="description" content="低精度训练是大模型训练中扩展模型大小，节约训练成本的最关键技术之一。"><meta itemprop="datePublished" content="2023-11-23T00:00:00+00:00" />
<meta itemprop="dateModified" content="2023-11-23T00:00:00+00:00" />
<meta itemprop="wordCount" content="4150">
<meta itemprop="keywords" content="" /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="用FP8训练大模型有多香？微软：比BF16快64%，省42%内存"/>
<meta name="twitter:description" content="低精度训练是大模型训练中扩展模型大小，节约训练成本的最关键技术之一。"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->




  </head>
  <body>
    <div id="back-to-top"></div>

    <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">区块大全</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="/">主页</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="/post/">归档</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="/categories/">分类</a>
          
        
      </li>
    

    
  </ul>
</nav>


    
      






  <link rel="stylesheet" href="/lib/photoswipe/photoswipe.min.css" />
  <link rel="stylesheet" href="/lib/photoswipe/default-skin/default-skin.min.css" />




<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>

    

    

    


    <header id="header" class="header">
      <div class="logo-wrapper">
  <a href="/" class="logo">
    
      区块大全
    
  </a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="/">主页</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="/post/">归档</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="/categories/">分类</a>
          

        

      </li>
    

    
    

    
  </ul>
</nav>

    </header>

    <div id="mobile-panel">
      <main id="main" class="main bg-llight wallpaper">
        <div class="content-wrapper">
    <div id="content" class="content">
      <article class="post">
        
        <header class="post-header">
          <h1 class="post-title">用FP8训练大模型有多香？微软：比BF16快64%，省42%内存</h1>
          

          <div class="post-meta">
  <div class="post-meta-author">
    by
      佚名
    
  </div>

  <div class="post-meta-time">
    <time datetime="2023-11-23">
      2023-11-23
    </time>
  </div>

  


  <div class="post-meta__right">
    <span class="post-meta-more">
        约 4150 字 -
        预计阅读 9 分钟
      </span>

    <div class="post-meta-category">
        <a href="/categories/%E5%85%B6%E5%AE%83%E6%96%87%E7%AB%A0/"> 其它文章 </a>
          
      </div>


    
    


    
    
  </div>
</div>

        </header>

        
        <div class="post-content">
          <table>
    <thead>
        <tr>
            <th style="text-align:left">推荐平台</th>
            <th style="text-align:left">链接</th>
            <th style="text-align:left">平台介绍</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td style="text-align:left"><span style="white-space:nowrap">币安网</span></td>
            <td style="text-align:left"><span style="white-space:nowrap"><a
                        href="https://www.okbtc.cn/binance?ref=githubio">注册链接</a></span></td>
            <td style="text-align:left"><a
                    href="https://www.okbtc.cn/binance?ref=githubio">币安是全球领先的区块链生态系统，推出了一系列产品，其中包括最大的加密货币交易平台。我们的使命是在未来成为全球性加密货币基础架构供应商。</a>
            </td>
        </tr>
        <tr>
            <td style="text-align:left"><span style="white-space:nowrap">欧易OKX</span></td>
            <td style="text-align:left"><a href="https://www.okbtc.cn/okx?ref=githubio">注册链接</a></td>
            <td style="text-align:left"><a
                    href="https://www.okbtc.cn/okx?ref=githubio">欧易是全球著名的数字资产交易平台之一，主要面向全球用户提供比特币、莱特币、以太币等数字资产的币币和衍生品交易服务。</a>
            </td>
        </tr>
        <tr>
            <td style="text-align:left"><span style="white-space:nowrap">HTX火币</span></td>
            <td style="text-align:left"><a href="https://www.okbtc.cn/htx?ref=githubio">注册链接</a></td>
            <td style="text-align:left"><a
                    href="https://www.okbtc.cn/htx?ref=githubio">火币全球专业站，是火币集团旗下服务于全球专业交易用户的创新数字资产国际站，致力于发现优质的创新数字资产投资机会。</a>
            </td>
        </tr>
    </tbody>
</table>
<p>原文来源：<a href="https://mp.weixin.qq.com/s/ZnObBjyg0vxvRpby74DM1g">机器之心</a></p>
<p><img src="https://img.bibiqing.com/news/2023/1102/17_ppvznnyl79.png" alt="用FP8训练大模型有多香？微软：比BF16快64%，省42%内存"></p>
<p>图片来源：由<a href="https://www.wujieai.cc/">无界 AI</a>生成</p>
<blockquote>
<p>低精度训练是大模型训练中扩展模型大小，节约训练成本的最关键技术之一。相比于当前的 16 位和 32 位浮点混合精度训练，使用 FP8 8 位浮点混合精度训练能带来 2 倍的速度提升，节省 50% - 75% 的显存和 50% - 75% 的通信成本，而且英伟达最新一代卡皇 H100 自带良好的 FP8 硬件支持。但目前业界大模型训练框架对 FP8 训练的支持还非常有限。最近，微软提出了一种用于训练 LLM 的 FP8 混合精度框架 FP8-LM，将 FP8 尽可能应用在大模型训练的计算、存储和通信中，使用 H100 训练 GPT-175B 的速度比 BF16 快 64%，节省 42% 的内存占用。更重要的是：它开源了。   </p>
</blockquote>
<p>大型语言模型（LLM）具有前所未有的语言理解和生成能力，但是解锁这些高级的能力需要巨大的模型规模和训练计算量。在这种背景下，尤其是当我们关注扩展至 OpenAI 提出的超级智能 (Super Intelligence) 模型规模时，低精度训练是其中最有效且最关键的技术之一，其优势包括内存占用小、训练速度快，通信开销低。目前大多数训练框架（如 Megatron-LM、MetaSeq 和 Colossal-AI）训练 LLM 默认使用 FP32 全精度或者 FP16/BF16 混合精度。 </p>
<p>但这仍然没有推至极限：随着英伟达 H100 GPU 的发布，FP8 正在成为下一代低精度表征的数据类型。理论上，相比于当前的 FP16/BF16 浮点混合精度训练，FP8 能带来 2 倍的速度提升，节省 50% - 75% 的内存成本和 50% - 75% 的通信成本。 </p>
<p>尽管如此，目前对 FP8 训练的支持还很有限。英伟达的 Transformer Engine (TE)，只将 FP8 用于 GEMM 计算，其所带来的端到端加速、内存和通信成本节省优势就非常有限了。 </p>
<p>但现在微软开源的 FP8-LM FP8 混合精度框架极大地解决了这个问题：FP8-LM 框架经过高度优化，在训练前向和后向传递中全程使用 FP8 格式，极大降低了系统的计算，显存和通信开销。 </p>
<p><img src="https://img.bibiqing.com/news/2023/1102/ryhth54d1zvy7us" alt="用FP8训练大模型有多香？微软：比BF16快64%，省42%内存"></p>
<ul>
<li>论文地址：https://arxiv.org/abs/2310.18313</li>
<li>开源框架：https://github.com/Azure/MS-AMP</li>
</ul>
<p>实验结果表明，在 H100 GPU 平台上训练 GPT-175B 模型时， FP8-LM 混合精度训练框架不仅减少了 42% 的实际内存占用，而且运行速度比广泛采用的 BF16 框架（即 Megatron-LM）快 64%，比 Nvidia Transformer Engine 快 17%。而且在预训练和多个下游任务上，使用 FP8-LM 训练框架可以得到目前标准的 BF16 混合精度框架相似结果的模型。 </p>
<p>在给定计算资源情况下，使用 FP8-LM 框架能够无痛提升可训练的模型大小多达 2.5 倍。有研发人员在推特上热议：如果 GPT-5 使用 FP8 训练，即使只使用同样数量的 H100，模型大小也将会是 GPT-4 的 2.5 倍！ </p>
<p><img src="https://img.bibiqing.com/news/2023/1102/rv5nt93yv2f9rgq" alt="用FP8训练大模型有多香？微软：比BF16快64%，省42%内存"></p>
<p>Huggingface 研发工程师调侃：「太酷啦，通过 FP8 大规模训练技术，可以实现计算欺骗！」</p>
<p><img src="https://img.bibiqing.com/news/2023/1102/1nsd2ho0g57y5mu" alt="用FP8训练大模型有多香？微软：比BF16快64%，省42%内存"></p>
<p>FP8-LM 主要贡献： </p>
<ul>
<li>一个新的 FP8 混合精度训练框架。其能以一种附加方式逐渐解锁 8 位的权重、梯度、优化器和分布式训练，这很便于使用。这个 8 位框架可以简单直接地替代现有 16/32 位混合精度方法中相应部分，而无需对超参数和训练方式做任何修改。此外，微软的这个团队还发布了一个 PyTorch 实现，让用户可通过少量代码就实现 8 位低精度训练。  </li>
<li>一个使用 FP8 训练的 GPT 式模型系列。他们使用了新提出的 FP8 方案来执行 GPT 预训练和微调（包括 SFT 和 RLHF），结果表明新方法在参数量从 70 亿到 1750 亿的各种大小的模型都颇具潜力。他们让常用的并行计算范式都有了 FP8 支持，包括张量、流水线和序列并行化，从而让用户可以使用 FP8 来训练大型基础模型。他们也以开源方式发布了首个基于 Megatron-LM 实现的 FP8 GPT 训练代码库。 </li>
</ul>
<p><strong>FP8-LM 实现</strong>  </p>
<p>具体来说，对于使用 FP8 来简化混合精度和分布式训练的目标，他们设计了三个优化层级。这三个层级能以一种渐进方式来逐渐整合 8 位的集体通信优化器和分布式并行训练。优化层级越高，就说明 LLM 训练中使用的 FP8 就越多。 </p>
<p>此外，对于大规模训练（比如在数千台 GPU 上训练 GPT-175B），该框架能提供 FP8 精度的低位数并行化，包括张量、训练流程和训练的并行化，这能铺就通往下一代低精度并行训练的道路。 </p>
<p>张量并行化是将一个模型的各个层分散到多台设备上，从而将权重、梯度和激活张量的分片放在不同的 GPU 上。 </p>
<p>为了让张量并行化支持 FP8，微软这个团队的做法是将分片的权重和激活张量转换成 FP8 格式，以便线性层计算，从而让前向计算和后向梯度集体通信全都使用 FP8。 </p>
<p>另一方面，序列并行化则是将输入序列切分成多个数据块，然后将子序列馈送到不同设备以节省激活内存。 </p>
<p>如图 2 所示，在一个 Transformer 模型中的不同部分，序列并行化和张量并行化正在执行，以充分利用可用内存并提高训练效率。 </p>
<p><img src="https://img.youtocoin.com/news/2023/1102/qhaje5oomo0mej3" alt="用FP8训练大模型有多香？微软：比BF16快64%，省42%内存"></p>
<p>而对于 ZeRO（零冗余优化器 / Zero Redundancy Optimizer），却无法直接应用 FP8，因为其难以处理与 FP8 划分有关的缩放因子。因此针对每个张量的缩放因子应当沿着 FP8 的划分方式分布。 </p>
<p>为了解决这个问题，研究者实现了一种新的 FP8 分配方案，其可将每个张量作为一个整体分散到多台设备上，而不是像 ZeRO 方法一样将其切分成多个子张量。该方法是以一种贪婪的方式来处理 FP8 张量的分配，如算法 1 所示。 </p>
<p><img src="https://img.bibiqing.com/news/2023/1102/nc29npfy6v8vgu8" alt="用FP8训练大模型有多香？微软：比BF16快64%，省42%内存"></p>
<p>具体来说，该方法首先根据大小对模型状态的张量排序，然后根据每个 GPU 的剩余内存大小将张量分配到不同的 GPU。这种分配遵循的原则是：剩余内存更大的 GPU 更优先接收新分配的张量。通过这种方式，可以平滑地沿张量分配张量缩放因子，同时还能降低通信和计算复杂度。图 3 展示了使用和不使用缩放因子时，ZeRO 张量划分方式之间的差异。 </p>
<p><img src="https://img.bibiqing.com/news/2023/1102/i53stg7dvsd644d" alt="用FP8训练大模型有多香？微软：比BF16快64%，省42%内存"></p>
<p>使用 FP8 训练 LLM 并不容易。其中涉及到很多挑战性问题，比如数据下溢或溢出；另外还有源自窄动态范围的量化错误和 FP8 数据格式固有的精度下降问题。这些难题会导致训练过程中出现数值不稳定问题和不可逆的分歧问题。为了解决这些问题，微软提出了两种技术：精度解耦（precision decoupling）和自动缩放（automatic scaling），以防止关键信息丢失。 </p>
<p><strong>精度解耦</strong>  </p>
<p>精度解耦涉及到解耦数据精度对权重、梯度、优化器状态等参数的影响，并将经过约简的精度分配给对精度不敏感的组件。 </p>
<p>针对精度解耦，该团队表示他们发现了一个指导原则：梯度统计可以使用较低的精度，而主权重必需高精度。 </p>
<p>更具体而言，一阶梯度矩可以容忍较高的量化误差，可以配备低精度的 FP8，而二阶矩则需要更高的精度。这是因为在使用 Adam 时，在模型更新期间，梯度的方向比其幅度更重要。具有张量缩放能力的 FP8 可以有效地将一阶矩的分布保留成高精度张量，尽管它也会导致精度出现一定程度的下降。由于梯度值通常很小，所以为二阶梯度矩计算梯度的平方可能导致数据下溢问题。因此，为了保留数值准确度，有必要分配更高的 16 位精度。 </p>
<p>另一方面，他们还发现使用高精度来保存主权重也很关键。其根本原因是在训练过程中，权重更新有时候会变得非常大或非常小，对于主权重而言，更高的精度有助于防止权重更新时丢失信息，实现更稳定和更准确的训练。 </p>
<p>在该实现中，主权重有两个可行选项：要么使用 FP32 全精度，要么使用带张量缩放的 FP16。带张量缩放的 FP16 的优势是能在无损于准确度的前提下节省内存。因此，新框架的默认选择是使用带张量缩放的 FP16 来存储优化器中的主权重。在训练中，对于 FP8 混合精度优化器，每个参数需要 6 个字节的内存： </p>
<p><img src="https://img.bibiqing.com/news/2023/1102/pep2z88635fo26s" alt="用FP8训练大模型有多香？微软：比BF16快64%，省42%内存"></p>
<p>相比于之前的解决方案，这种新的低位数优化器可将内存足迹降低 2.6 倍。值得说明的是：这是首个用于 LLM 训练的 FP8 优化器。实验表明 FP8 优化器能在从 1.25 亿到 1750 亿参数的各种模型大小下保持模型准确度。 </p>
<p><strong>自动缩放</strong>  </p>
<p>自动缩放是为了将梯度值保存到 FP8 数据格式的表征范围内，这需要动态调整张量缩放因子，由此可以减少 all-reduce 通信过程中出现的数据下溢和溢出问题。 </p>
<p>具体来说，研究者引入了一个自动缩放因子 μ，其可以在训练过程中根据情况变化。 </p>
<p><strong>实验结果</strong></p>
<p>为了验证新提出的 FP8 低精度框架，研究者实验了用它来训练 GPT 式的模型，其中包括预训练和监督式微调（SFT）。实验在 Azure 云计算最新 NDv5 H100 超算平台上进行。 </p>
<p>实验结果表明新提出的 FP8 方法是有效的：相比于之前广泛使用 BF16 混合精度训练方法，新方法优势明显，包括真实内存用量下降了 27%-42%（比如对于 GPT-7B 模型下降了 27%，对于 GPT-175B 模型则下降了 42%）；权重梯度通信开销更是下降了 63%-65%。 </p>
<p><img src="https://img.bibiqing.com/news/2023/1102/110nkb1yyrsgzbe" alt="用FP8训练大模型有多香？微软：比BF16快64%，省42%内存"></p>
<p>不修改学习率和权重衰减等任何超参数，不管是预训练任务还是下游任务，使用 FP8 训练的模型与使用 BF16 高精度训练的模型的表现相当。值得注意的是，在 GPT-175B 模型的训练期间，相比于 TE 方法，在 H100 GPU 平台上，新提出的 FP8 混合精度框架可将训练时间减少 17%，同时内存占用少 21%。更重要的是，随着模型规模继续扩展，通过使用低精度的 FP8 还能进一步降低成本，如图 1 所示。 </p>
<p><img src="https://img.bibiqing.com/news/2023/1102/sypyr855nlq61tv" alt="用FP8训练大模型有多香？微软：比BF16快64%，省42%内存"></p>
<p>对于微调，他们使用了 FP8 混合精度来进行指令微调，并使用了使用人类反馈的强化学习（RLHF）来更好地将预训练后的 LLM 与终端任务和用户偏好对齐。 </p>
<p><img src="https://img.bibiqing.com/news/2023/1102/4uzdew1rnke1bwq" alt="用FP8训练大模型有多香？微软：比BF16快64%，省42%内存"></p>
<p>结果发现，在 AlpacaEval 和 MT-Bench 基准上，使用 FP8 混合精度微调的模型与使用半精度 BF16 微调的模型的性能相当，而使用 FP8 的训练速度还快 27%。此外，FP8 混合精度在 RLHF 方面也展现出了巨大的潜力，该过程需要在训练期间加载多个模型。通过在训练中使用 FP8，流行的 RLHF 框架 AlpacaFarm 可将模型权重减少 46%，将优化器状态的内存消耗减少 62%。这能进一步展现新提出的 FP8 低精度训练框架的多功能性和适应性。 </p>
<p>他们也进行了消融实验，验证了各组件的有效性。 </p>
<p>可预见，FP8 低精度训练将成为未来大模型研发的新基建。</p>
<table>
    <thead>
        <tr>
            <th style="text-align:left">推荐平台</th>
            <th style="text-align:left">链接</th>
            <th style="text-align:left">平台介绍</th>
        </tr>
    </thead>
    <tbody>
        <tr>
            <td style="text-align:left"><span style="white-space:nowrap">Gate芝麻开门</span></td>
            <td style="text-align:left"><span style="white-space:nowrap"><a
                        href="https://www.okbtc.cn/gateio?ref=githubio">平台介绍</a></span></td>
            <td style="text-align:left"><a
                    href="https://www.okbtc.cn/gateio?ref=githubio">Gate.io芝麻开门创立于2013年，是全球真实交易量TOP10的加密货币交易平台，向全球数千万用户提供安全可靠、真实透明的数字资产交易服务。</a>
            </td>
        </tr>
        <tr>
            <td style="text-align:left"><span style="white-space:nowrap">Bitget</span></td>
            <td style="text-align:left"><a href="https://www.okbtc.cn/bitget?ref=githubio">注册链接</a></td>
            <td style="text-align:left"><a
                    href="https://www.okbtc.cn/bitget?ref=githubio">Bitget的背后是一群区块链技术的早期接受者，也是区块链未来发展的信仰者，一直致力于提供安全、一站式的交易解决方案，帮助用户更聪明地交易。</a>
            </td>
        </tr>
        <tr>
            <td style="text-align:left"><span style="white-space:nowrap">Bybit</span></td>
            <td style="text-align:left"><a href="https://www.okbtc.cn/bybit?ref=githubio">注册链接</a></td>
            <td style="text-align:left"><a
                    href="https://www.okbtc.cn/bybit?ref=githubio">Bybit通过数字资产与传统金融的结合，引领数字资产的生态发展。提供一流的流动性，致力于打造业内最安全、公平、高效及人性化的交易服务平台。</a>
            </td>
        </tr>
        <tr>
            <td style="text-align:left"><span style="white-space:nowrap">派网</span></td>
            <td style="text-align:left"><a href="https://www.okbtc.cn/pionex?ref=githubio">注册链接</a></td>
            <td style="text-align:left"><a
                    href="https://www.okbtc.cn/pionex?ref=githubio">派网提供多样化的量化交易机器人，用户可依照自身交易需求和策略选择最适合的机器人。 同时派网也提供合约交易与合约网格机器人，给予更方便的合约交易体验。</a>
            </td>
        </tr>
    </tbody>
</table>

        </div>

        
        



        
        


        <footer class="post-footer">
          


          
          <nav class="post-nav">
            
              <a class="prev" href="/post/63762/">
                
                <i class="iconfont">
                  <svg  class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417 757.434875 204.940602c11.338233-12.190647 11.035334-32.285311-0.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-0.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891 0.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"></path>
</svg>

                </i>
                <span class="prev-text nav-default">英伟达Q3业绩爆炸式增长！黄仁勋：AI时代正在起飞</span>
                <span class="prev-text nav-mobile">上一篇</span>
              </a>
            
              <a class="next" href="/post/62401/">
                <span class="next-text nav-default">有人要做「AI 科学家」，每天刷上万篇论文还能提出假设，前谷歌董事长背书</span>
                <span class="prev-text nav-mobile">下一篇</span>
                
                <i class="iconfont">
                  <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311 0.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889 0.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-0.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"></path>
</svg>

                </i>
              </a>
          </nav>
        </footer>
      </article>

      
      
        
      


      
      


    </div>

    
    


  </div>

      </main>

      <footer id="footer" class="footer">
        <div class="icon-links">
  

<a href="https://www.okbtc.cn/binance?ref=githubio" class="iconfont">
  <img src="/image/logo/binance.png" width="36px" height="36px" alt="binance">
</a>

<a href="https://www.okbtc.cn/okx?ref=githubio" class="iconfont">
  <img src="/image/logo/okx.png" width="36px" height="36px" alt="okx">
</a>

<a href="https://www.okbtc.cn/htx?ref=githubio" class="iconfont">
  <img src="/image/logo/htx.png" width="36px" height="36px" alt="htx">
</a>

<a href="https://www.okbtc.cn/gateio?ref=githubio" class="iconfont">
  <img src="/image/logo/gateio.png" width="36px" height="36px" alt="gateio">
</a>

<a href="https://www.okbtc.cn/bitget?ref=githubio" class="iconfont">
  <img src="/image/logo/bitget.png" width="36px" height="36px" alt="bitget">
</a>

<a href="https://www.okbtc.cn/bybit?ref=githubio" class="iconfont">
  <img src="/image/logo/bybit.png" width="36px" height="36px" alt="bybit">
</a>

<a href="https://www.okbtc.cn/pionex?ref=githubio" class="iconfont">
  <img src="/image/logo/pionex.png" width="36px" height="36px" alt="pionex">
</a>



</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - <a class="theme-link" href="https://github.com/xianmin/hugo-theme-jane">Jane</a>
  </span>

  <span class="copyright-year">
    &copy;
    2023
    <span class="heart">
      
      <i class="iconfont">
        <svg class="icon" viewBox="0 0 1025 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="14" height="14">
  <path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7 0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1 0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2 0.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2 0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3 0.1-42.5-8-83.6-24-122.2z"
   fill="#8a8a8a"></path>
</svg>

      </i>
    </span><span class="author">
        coin
        
      </span></span>

  
  

  
</div>

      </footer>

      <div class="button__back-to-top">
        <a href="#back-to-top">
          <i class="iconfont">
            
            <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="35" height="35">
  <path d="M510.866688 227.694839 95.449397 629.218702l235.761562 0-2.057869 328.796468 362.40389 0L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777l894.052392 0 0 131.813095L63.840492 195.775872 63.840492 63.962777 63.840492 63.962777zM63.840492 63.962777"></path>
</svg>

          </i>
        </a>
      </div>
    </div>
    
<script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>




<script type="text/javascript" src="/js/main.8b200667dc4d9618390c41639250b9c29e57ce80707352fa95af6cb67cf2371a.js" integrity="sha256-iyAGZ9xNlhg5DEFjklC5wp5XzoBwc1L6la9stnzyNxo=" crossorigin="anonymous"></script>












  
    <script type="text/javascript" src="/lib/photoswipe/photoswipe.min.js"></script>
    <script type="text/javascript" src="/lib/photoswipe/photoswipe-ui-default.min.js"></script>
  











<script>
  var remark_config = {
    host: 'https:\/\/remark42.example.com',
    site_id: 'remark',
    components: [
	    'embed',
    ],
  }
  !function(e,n){for(var o=0;o<e.length;o++){var r=n.createElement("script"),c=".js",d=n.head||n.body;"noModule"in r?(r.type="module",c=".mjs"):r.async=!0,r.defer=!0,r.src=remark_config.host+"/web/"+e[o]+c,d.appendChild(r)}}(remark_config.components||["embed"],document);
</script>







  </body>
</html>
